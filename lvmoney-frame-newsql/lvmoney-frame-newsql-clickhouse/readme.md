过期日志的清理，我们通过一个定时任务每天删除过期日志的partition  
ClickHouse的监控，使用ClickHouse-exporter+VictoriaMetrics+Grafana的实现  
· 大多数是读请求
· 数据总是以相当大的批(> 1000 rows)进行写入
· 不修改已添加的数据
· 每次查询都从数据库中读取大量的行，但是同时又仅需要少量的列
· 宽表，即每个表包含着大量的列
· 较少的查询(通常每台服务器每秒数百个查询或更少)
· 对于简单查询，允许延迟大约50毫秒
· 列中的数据相对较小： 数字和短字符串(例如，每个URL 60个字节)
· 处理单个查询时需要高吞吐量（每个服务器每秒高达数十亿行）
· 事务不是必须的
· 对数据一致性要求低
· 每一个查询除了一个大表外都很小
· 查询结果明显小于源数据，换句话说，数据被过滤或聚合后能够被盛放在单台服务器的内存中

mysql 增量同步到clickhouse，这里有一个思考系统日志，交易日志，订单等不变的数据是否可以同步到clickhouse，
由于用户经常查询和操作可能是最近的数据，可以把这部分变更的有事务要求的数据放到mysql中，然后把mysql数据同步到clickhouse中，
这样来减轻关系型数据库的性能瓶颈  

各种日志数据我们可以用flume同步到clickhouse来统一管理和做用户行为分析  

面对错综复杂的数据源我们似乎可以使用flink来把数据统一归集到clickhouse  


mysql增量同步到clickhouse已经实现  

flume的clickhouse sink 待测试